\section{Scanner}
A scanners job is to analyse an input for lexical errors. The techniques required for doing a lexical analysis are more simple than the techniques required for syntactical analysis, and it is easier to optimize the lexical analysis phase if it is kept separated from the parser. We have decided to separate the lexical- and syntactical analysis like most other compilers or interpreters do. This section will cover the lexical analysis, which is performed by a scanner\cite[p.189]{sebesta2013}

A scanner can be hard coded by hand or generated using existing tools, e.g. JFLex (for Java). When using tools like JFlex, you must define tokens to match input using regular expressions. If a programming language is complex, writing a scanner by hand can be very time consuming and error prone. On the other hand, learning how to use a tool like JFLex can also take a lot of time. Tools like Flex will often create faster scanners because they have incorporated many smart tricks to tweak the performance of the scanner generated\cite[p.116]{fischer2009}.
 We have been using JFLex in a course running beside this project, which is the reason why we have chosen to hand code the scanner for this project, to experience how that method works as well.
 
A finite automata can be used to recognise tokens specified using regular expressions. The automata can be table driven where each pair of a state and an input character matches an element in the table which points to the next state. A list of accept states tells in what states a string will be accepted and which token it will be accepted as. However, one can also code a DFA with explicit control, where the transition table that defines the DFA's actions is not declared explicit, but is incorporated as the control logic. The table driven DFA is often generated by tools that converts regular expression into the table used by the DFA. We have chosen to use the finite automata approach using the explicit control form, because most of us have no experience in writing a scanner and we think we will gain more from the explicit control form method rather than using a tool to do so.
\cite[p.94]{fischer2009}.

Our scanner takes a raw source code for a program written in \productname{} as input. It validates the lexically correctness of a \productname{} program. The scanner tries to find tokens existing in \productname{} from the input. If an input is met which can not be recognized as a valid token, the source code is not a valid program in \productname{}.
\todo{Error handling?}
 The strings which are converted to tokens are called lexemes. Many lexemes can be converted to the same type of token. Programs will typically contain many different identifiers, which in \productname{} all will have a \tokenref{ID} instantiated. The name of the identifier will then be saved as value belonging the particular token. Consider the example of this single line taken from a chess game written in \productname{}:
\begin{lstlisting}
Black{ Pawn [A7 B7 C7 D7 E7 F7 G7 H7] }
\end{lstlisting}
The result of analysing the input can be seen in \tableref{table:lexemestotokens}. Tokens are needed for abstraction. When the parser later on will determine if the code respects the grammar of \productname{}, it is useful to have these abstractions. It makes it possible to describe that a list of \tokenref{COORD\_LIT}'s can be encapsulated between the characters ``[ ]'' without having to list all possible coordinate literals, which in fact are an infinite set, since the grammar of \productname{} allows proceeding in both dimensions after \textit{Z9}, namely \textit{Z10} and \textit{AA9}. However, like an identifier, the value of other token types, for instance coordinate literal is still kept since that information will be needed later for the subsequent parts of the interpreter.

\begin{figure}
\centering
\begin{tabular}{|l|l|}
        \hline
        Lexemes & Tokens             \\ \hline
        Black   & IDENTIFIER (Black) \\ 
        \{       & LBRACE             \\ 
        Pawn    & IDENTIFIER (Pawn)  \\ 
        $[$       & LBRACKET           \\ 
        A7      & COORD\_LIT (A7)     \\ 
        B7      & COORD\_LIT (B7)     \\ 
        C7      & COORD\_LIT (C7)     \\ 
        D7      & COORD\_LIT (D7)     \\ 
        E7      & COORD\_LIT (E7)     \\ 
        F7      & COORD\_LIT (F7)     \\ 
        G7      & COORD\_LIT (G7)     \\ 
        H7      & COORD\_LIT (H7)     \\ 
        $]$       & RBRACKET           \\ 
        \}       & RBRACE             \\
        \hline
\end{tabular}
\capt{Analysing an input stream for lexemes and tokens}\label{table:lexemestotokens}
\end{figure}

Our scanner contains 2 classes, \classref{Scanner} and \classref{Token}. \classref{Token} contains an enum named \classref{Type} that enumerates all the types of tokens in \productname{}. When a lexeme is found in the input stream, the scanner analyses which token type it belongs to. A new token is then instantiated and returned by the scanner. The constructor for \classref{Token} takes the arguments (\classref{Token.Type}, \textit{line}, \textit{offset}). The \textit{line} and \textit{offset} represent where in the source code the lexeme of any token where found. This is essential if an error is found, since we can then inform a programmer where in his source code he should look for the error.
 
When an input is to be analysed, the scanner looks at the first symbol of the input and determines which subfunction to jump to. 
This is where the explicit control of the DFA is seen. If we had used a table driven automata, the program would just update a variable \varref{currentState}. 
Instead this variable exists implicit as the call stack showing which subfunctions we have jumped into. In \lstref{lst:scan}, you can clearly see many functions with the name \methodref{isSomething()}, which simply returns if the next symbol in the input stream is ``Something''. \methodref{isWhitespace()} returns true if the next input is a whitespace.
While the condition is true, \methodref{pop()} dequeues the next symbol. Therefore, the while loop with \methodref{isWhitespace()} removes all initial white spaces before a next token is found. After that, if the scanner has reached the end of the input stream, it returns a \tokenref{EOF}. 
For all \methodref{isSomething()} functions, except the \methodref{isWhitespace()} function, a token will be returned based on some evaluations the subfunction is responsible for.
For example, if the first symbol of a lexeme is an upper-case character, the function \methodref{scanUppercase()} is responsible for determining whether the lexeme is an \tokenref{identifier} or a \tokenref{direction}, because they are the only tokens starting with an upper-case character in \productname{}. Two variables, \typeref{int} \varref{offset} and \typeref{int} \varref{line} keeps track of where in the source file the next input character is taken from. The function \methodref{pop()} will pop the first character from the input stream and assign the value of the new first character to the variable \varref{nextChar}. The \methodref{isSomething()} functions uses that \varref{nextChar} to check what kind of character the next one is.
The \methodref{pop()} function will additionally increment \varref{offset} by one. If the next input symbol is a whitespace, it assigns zero to  \varref{offset} and increments \varref{line} by one. If an input symbol is met which was not expected, an error is outputted.
 \todo{What to do about errors here?}
\begin{figure}
\begin{lstlisting}
public Token scan() throws Exception {
  while (isWhitespace()) {
    pop();
  }
  if (isEof()) {
    return token(Type.EOF);
  }
  if (isDigit()) {
    return scanNumeric();
  }
  if (isUppercase()) {
    return scanUppercase();
  }
  if (isOperator()) {
    return scanOperator();
  }
  if (isLowercase()) {
    return scanKeyword();
  }
  if (current() == '"') {
    return scanString();
  }
  if (current() == '$') {
    return scanVar();
  }
  throw new ScannerError("Unidentified character: " + current(), token(Type.EOF));
}
\end{lstlisting}
\capt{The scan() function from the \productname{} scanner.}\label{lst:scan}
\end{figure}
