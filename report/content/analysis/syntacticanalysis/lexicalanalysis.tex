\subsection{Lexical analysis}
\label{sec:lexicalanalysis}
Before the syntax analysis can be performed, a scanner has to do a lexical analysis. The scanner takes an input stream of characters and outputs a stream of tokens. 

The lowest level syntactic units of a language is called lexemes. A language's formal description does not often include these. They are instead described by a lexical specification, regular expressions i.e., separated from the syntactic specification\cite[p. 135]{sebesta2013}. Typical lexemes for a programming language includes integer literals, operators and special keywords like \textit{if} and \textit{while}. If both \textit{\$a} and \textit{\$b} are lexemes describing a variable and \textit{102} and \textit{42} are lexemes describing an integer, then \textit{\$a} and \textit{\$b} or \textit{102} and \textit{42} can typically be used interchangeably and still give a meaningful program. Therefore the lexemes are grouped into tokens. The name of a variable or the value of an integer is preserved when tokenising. The tokens are an abstraction that makes it easier to analyse if correct syntax of the language. An example of the grouping of lexemes into tokens can be seen by \tableref{table:lexandtokens}. After 

\tab[4cm]{lexandtokens}{1}{Lexemes and their corresponding token group.}
		    {               }
{Lexemes   }{\textbf{Tokens}}{
\tabrow{\$a}{var(a) 		}
\tabrow{=  }{assign 		}
\tabrow{3  }{int(3) 		}
\tabrow{\$b}{var(b) 		}
\tabrow{+  }{plus   		}
\tabrow{4  }{int(4) 		}
\tabrow{\$a}{var(a) 		}
}